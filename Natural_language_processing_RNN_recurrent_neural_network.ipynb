{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26eee3fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afa99590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "vocab_size = 88584\n",
    "maxlen = 250\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb56cfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce97c8",
   "metadata": {},
   "source": [
    "# More Processing\n",
    "\n",
    "1. If the review is greater than 250  words then trim off the extra words\n",
    "2. if the review is less than 250 words add the necessary amount  of 's to make it equal to 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d33e0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     1,   194,\n",
       "        1153,   194,  8255,    78,   228,     5,     6,  1463,  4369,\n",
       "        5012,   134,    26,     4,   715,     8,   118,  1634,    14,\n",
       "         394,    20,    13,   119,   954,   189,   102,     5,   207,\n",
       "         110,  3103,    21,    14,    69,   188,     8,    30,    23,\n",
       "           7,     4,   249,   126,    93,     4,   114,     9,  2300,\n",
       "        1523,     5,   647,     4,   116,     9,    35,  8163,     4,\n",
       "         229,     9,   340,  1322,     4,   118,     9,     4,   130,\n",
       "        4901,    19,     4,  1002,     5,    89,    29,   952,    46,\n",
       "          37,     4,   455,     9,    45,    43,    38,  1543,  1905,\n",
       "         398,     4,  1649,    26,  6853,     5,   163,    11,  3215,\n",
       "       10156,     4,  1153,     9,   194,   775,     7,  8255, 11596,\n",
       "         349,  2637,   148,   605, 15358,  8003,    15,   123,   125,\n",
       "          68, 23141,  6853,    15,   349,   165,  4362,    98,     5,\n",
       "           4,   228,     9,    43, 36893,  1157,    15,   299,   120,\n",
       "           5,   120,   174,    11,   220,   175,   136,    50,     9,\n",
       "        4373,   228,  8255,     5, 25249,   656,   245,  2350,     5,\n",
       "           4,  9837,   131,   152,   491,    18, 46151,    32,  7464,\n",
       "        1212,    14,     9,     6,   371,    78,    22,   625,    64,\n",
       "        1382,     9,     8,   168,   145,    23,     4,  1690,    15,\n",
       "          16,     4,  1355,     5,    28,     6,    52,   154,   462,\n",
       "          33,    89,    78,   285,    16,   145,    95])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = sequence.pad_sequences(train_data, maxlen )\n",
    "test_data = sequence.pad_sequences(test_data, maxlen)\n",
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fc516",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "Now it's time to create the model. we'll use a word embedding layer as the first layer in our model  and add LSTM layer afterwards that feeds into a dense node to get our predicted sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1694393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2843041 (10.85 MB)\n",
      "Trainable params: 2843041 (10.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 32),\n",
    "    tf.keras.layers.LSTM(32),  #32 means the output dimension of the generated vector\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc1f85",
   "metadata": {},
   "source": [
    "# Training model\n",
    "\n",
    "Now it's time to compile and train the model\n",
    "use loss function = binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "619d4078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YASHASVI\\Desktop\\data science\\env\\Lib\\site-packages\\keras\\src\\backend.py:5820: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 58s 83ms/step - loss: 0.4487 - accuracy: 0.7826 - val_loss: 0.2940 - val_accuracy: 0.8828\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.2618 - accuracy: 0.8989 - val_loss: 0.2904 - val_accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_labels, epochs = 2, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b055566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2978 - accuracy: 0.8770\n",
      "[0.29783228039741516, 0.8769599795341492]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba08018",
   "metadata": {},
   "source": [
    "# Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09766601",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()  # for getting the index of every word in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c5d4b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text):\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    \n",
    "    return sequence.pad_sequences([tokens], maxlen)[0]\n",
    "\n",
    "text = 'that movie was just amazing , so amazing' \n",
    "\n",
    "encode = encode_text(text)\n",
    "print(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c5b6d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "# while were at it lets make a decode function  \n",
    "\n",
    "reverse_word_index = {value : key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    pad = 0\n",
    "    text = \"\"\n",
    "    \n",
    "    for num in integers:\n",
    "        if num != pad:\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "     \n",
    "    return text[:-1]\n",
    "print(decode_integers(encode))    #reversing the integer back to the word or lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f61561",
   "metadata": {},
   "source": [
    "# Now lets take review to as input and predict it's review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90b20c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much did you like this movvie : bhaut hi ghatiya movie thi bhai mat dekhna kabhi bhi bakwas nikli ek number ki\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Probability and Predicted class of this Statement is 0.4441061019897461 and 0.\n",
      "Review of this movie is Not so good\n"
     ]
    }
   ],
   "source": [
    "# Now time to make prediction\n",
    "\n",
    "def predict():\n",
    "    encoded_text = encode_text(input(\"How much did you like this movvie : \"))\n",
    "    \n",
    "    pred = np.zeros((1, 250))  #our model accept this shape \n",
    "    \n",
    "    pred[0] = encoded_text     # now pred ka (x, y) x me enocde_text save kiyaa\n",
    "    \n",
    "    result = model.predict(pred)\n",
    "    prob = result[0][0]\n",
    "    return prob\n",
    "\n",
    "# positive_review = 'That movie was so awesome ! I really loved it and would watch it again because it was amazingly great'\n",
    "\n",
    "prob = predict()\n",
    "\n",
    "# # negative_review = \"That movie sucked . I hated it and wouldn't watch it again , was one off the worst ever i watched\"\n",
    "\n",
    "# # predict(negative_review)\n",
    "\n",
    "# th = 0.5\n",
    "if prob>0.5:\n",
    "    class_ = 1\n",
    "    review = 'Good'\n",
    "elif prob<0.5:\n",
    "    class_ = 0\n",
    "    review = 'Not so good'\n",
    "print(f\"Probability and Predicted class of this Statement is {prob} and {class_}.\")\n",
    "print(f\"Review of this movie is {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf8f6f",
   "metadata": {},
   "source": [
    "# Lower the number of prediction more negative prediction it is and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9789ca",
   "metadata": {},
   "source": [
    "# Now what if we get so many review at a time we need to predict their class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1962845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the responses on this moviebhaut hi ghatiya movie thi bhai mat dekhna kabhi bhi bakwas nikli ek number ki\n",
      "Enter the responses on this movievery nice movie evry one should watch it it's wonnderful\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Probability and Predicted class of this Statement is 0.5090075135231018 and 1.\n",
      "Review of this movie is Good\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for i in range(2):\n",
    "    texts.append(input(\"Enter the responses on this movie\"))\n",
    "\n",
    "\n",
    "# Now time to make prediction\n",
    "\n",
    "def predict():\n",
    "    for text in texts:\n",
    "        \n",
    "        encoded_text = encode_text(text)\n",
    "\n",
    "        pred = np.zeros((1, 250))  #our model accept this shape \n",
    "\n",
    "        pred[0] = encoded_text     # now pred ka (x, y) x me enocde_text save kiyaa\n",
    "\n",
    "        result = model.predict(pred)\n",
    "        prob = result[0][0]    \n",
    "    return prob\n",
    "\n",
    "# positive_review = 'That movie was so awesome ! I really loved it and would watch it again because it was amazingly great'\n",
    "\n",
    "prob = predict()\n",
    "\n",
    "# # negative_review = \"That movie sucked . I hated it and wouldn't watch it again , was one off the worst ever i watched\"\n",
    "\n",
    "# # predict(negative_review)\n",
    "\n",
    "# th = 0.5\n",
    "if prob>0.5:\n",
    "    class_ = 1\n",
    "    review = 'Good'\n",
    "elif prob<0.5:\n",
    "    class_ = 0\n",
    "    review = 'Not so good'\n",
    "print(f\"Probability and Predicted class of this Statement is {prob} and {class_}.\")\n",
    "print(f\"Review of this movie is {review}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f6b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
