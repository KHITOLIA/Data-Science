{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b99ca3-5edf-46f3-af53-832fb2e4a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yashasvi\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\yashasvi\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yashasvi\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yashasvi\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yashasvi\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yashasvi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0acf68c6-20cc-42be-b142-09bea68b4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello welcome, to Krish Naik's NLP tutorial.\n",
    "please watch the entire course! to become an expert in NLP\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "319f7e10-896c-443a-ad97-630d3e036f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome, to Krish Naik's NLP tutorial.\n",
      "please watch the entire course! to become an expert in NLP\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8874817-5f91-40a5-b9f5-c8f78a9fbc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "## Sentence --> paragraphs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize  # convert the paragragh into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "636ba84b-f914-41dd-a1ea-86020f546c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14ea0b7b-7011-42d0-87ae-89c350f1b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word Tokenize\n",
    "## paragragh --> words\n",
    "## sentence --> words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3326504-781e-483e-b5de-59b6a290ff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorial',\n",
       " '.',\n",
       " 'please',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus) # every word will be seperated into single element in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a90dc83-7498-43c4-a9b1-97a951c3b2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'tutorial', '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e26a37f-6bc8-4580-9bbf-8819f90744b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'tutorial', '.']\n",
      "['please', 'watch', 'the', 'entire', 'course', '!']\n",
      "['to', 'become', 'an', 'expert', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39555a1e-758e-494a-adaa-60ebf25bce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be87d9f8-adf4-4c32-b4f3-cb3dd87a0734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLP',\n",
       " 'tutorial',\n",
       " '.',\n",
       " 'please',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus) # this will take punctuations also from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e87c459f-5ada-4662-be64-85ed503465d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17de7116-c679-4171-b684-54b2eb4c70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8db4f3f9-dc12-4e0a-be46-c8415ddde0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " \"'s\",\n",
       " 'NLP',\n",
       " 'tutorial.',\n",
       " 'please',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus) # Full stop or apaustoffe s is considered and only last full stop is seperated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5c83b-70cd-4334-ba10-e5bbb71380e8",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to it's word stem that affixes to suffix and prefixes or to the roots of words known as a lemma. Stemming is important in Natural Language Understanding and NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48c2c8cb-5b22-47a8-ac20-c2d6c8f0ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification problem \n",
    "## comment of product is a positive review or negative review\n",
    "## Reviews --> eating , eat , eaten, [going, gone, goes, go]  breakdown the word to it's root stem like go and eat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc9afafd-5020-4cef-9b93-164c40c6b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e2eb493-f6dc-4f21-9bbd-f17b85114c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming Techniques \n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bcb1ec87-d992-408f-9ff7-9ec9b6f7104a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'militari'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "stemming.stem('Military')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ef61237-0145-41a6-ba5e-42ca9bb4b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eats ----> eat\n",
      "eaten ----> eaten\n",
      "writing ----> write\n",
      "writes ----> write\n",
      "programming ----> program\n",
      "programs ----> program\n",
      "history ----> histori\n",
      "finally ----> final\n",
      "finalized ----> final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word,\"---->\",stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "27d29d47-3e21-4141-9fb7-678df36ef958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e079809-3e97-4e27-a32a-9b4153553732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('sitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66c57a-2fa5-497e-a245-3499278fe739",
   "metadata": {},
   "source": [
    "# RegexpStemmer Class \n",
    "NLTK has RegexpStemmer class with the help of which we can easily Implement Regular Expression Stemmer Algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16214f83-879f-4138-a41a-df9cf4c532e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47c84a28-3dae-4e68-8e04-6fd901886901",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$',min=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "64ddf922-0d3b-4cfd-aa8b-09845fc497d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d3c06814-c07d-4a87-aa08-cf8c495fee43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a57759-6cbc-4831-91c7-c712e63fa910",
   "metadata": {},
   "source": [
    "# Snowball Stemmer\n",
    "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5691740e-adea-47b9-9b99-bfec1e2360e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6d804ae-d55f-469f-a9c8-13b84f65d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "95f3e1d5-ae6c-4573-9e05-5955d5a8672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fb11f-77ca-4010-a5c2-be9b83467ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
