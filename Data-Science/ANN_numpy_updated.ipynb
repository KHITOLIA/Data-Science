{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.446891 -0.013397  0.232645  2.156649  1.652923 -0.210531 -0.662227   \n",
      "1      0.243747 -1.144175 -0.622214 -0.661979 -0.373315  0.520313 -1.307954   \n",
      "2     -1.417799 -0.088833 -0.647181 -1.141497 -1.321100  0.686798 -1.045569   \n",
      "3      0.656784  0.381201 -1.039011  1.285315  0.808324  1.428519 -1.144367   \n",
      "4      1.098520  1.706769 -1.030370  2.001009  2.751551  1.202229 -0.941066   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995  0.146324 -0.618555  0.547413  0.851301  0.838880 -1.248402  0.698199   \n",
      "19996 -2.227923 -0.327196  1.258849 -0.945254  3.925262  0.316686 -1.061159   \n",
      "19997 -0.800405  1.477997  0.090057 -0.973997 -0.204970  0.224539  0.302265   \n",
      "19998 -1.217344 -0.785401 -0.413781 -0.335324  0.181671 -1.685733 -0.437981   \n",
      "19999  1.601005 -0.890398 -1.425049  0.887501  0.167205 -0.395042 -0.512554   \n",
      "\n",
      "             7         8         9         10        11        12        13  \\\n",
      "0      0.705144  1.434684 -0.445750 -0.178501 -0.905503 -0.594099 -0.728586   \n",
      "1      1.639481 -2.838816  0.331636  0.100261 -1.173067 -0.079774  2.194899   \n",
      "2     -0.376153 -2.272447 -0.669733 -1.070732  1.091993  0.692293  0.937617   \n",
      "3     -0.609958  0.983676  0.060917  0.471647 -0.594279  2.104027 -1.113597   \n",
      "4      1.965282  2.979131  1.402483 -1.432987  0.400322 -0.839851 -1.580974   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995 -0.545033  0.994348 -0.054620  0.581239 -1.842074 -0.552879 -0.102096   \n",
      "19996  1.846706  3.692157  0.698035 -0.184239 -0.546036 -0.457919 -1.030581   \n",
      "19997  0.361146 -1.906636  1.381875  0.034366  1.690199 -0.901569  2.183842   \n",
      "19998 -1.176626 -1.434436 -0.880986  2.117703 -2.136842 -1.185261  1.547704   \n",
      "19999  0.050777 -0.034945 -0.289219  0.639189  0.981751 -0.110296 -0.187214   \n",
      "\n",
      "             14        15        16        17        18        19  \n",
      "0     -0.627002 -0.240520  0.131696 -0.945878  1.840922 -0.279024  \n",
      "1      0.242839  0.412164 -0.328714  0.902931 -1.418728 -0.290139  \n",
      "2      0.262142  0.399238  0.614614  1.209465 -1.526386  0.337771  \n",
      "3     -0.188799  1.226253 -1.067763 -0.350275 -0.746426 -1.258696  \n",
      "4     -0.659587  2.292688  0.494188 -1.583187 -2.468254 -0.469648  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "19995  0.307538 -1.267832 -0.165875 -0.717397 -0.808588  0.796346  \n",
      "19996  2.406014 -0.978943 -0.086879 -1.964453 -0.875574  1.535930  \n",
      "19997  0.831008  0.036308  0.516088  0.378670  0.110968 -0.176363  \n",
      "19998  1.174712 -0.429117  0.132068  0.249277 -0.941496  1.661661  \n",
      "19999 -0.226811 -0.547772 -0.006706 -0.023773  1.220809  1.489330  \n",
      "\n",
      "[20000 rows x 20 columns]\n",
      "       0\n",
      "0      3\n",
      "1      2\n",
      "2      2\n",
      "3      3\n",
      "4      3\n",
      "...   ..\n",
      "19995  3\n",
      "19996  1\n",
      "19997  2\n",
      "19998  2\n",
      "19999  0\n",
      "\n",
      "[20000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/bhatt/OneDrive/Desktop/whisper-main/ANN_Data/data1.csv', header=None)\n",
    "print(data)\n",
    "labels=pd.read_csv('C:/Users/bhatt/OneDrive/Desktop/whisper-main/ANN_Data/label1.csv',header=None)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 20)\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "data=data.values\n",
    "npLabels=labels.values\n",
    "print(data.shape)\n",
    "print(npLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(npLabels))\n",
    "print(np.min(npLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(npLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "num=np.max(npLabels)+1\n",
    "oneHot=np.eye(num)[npLabels]\n",
    "print(oneHot.ndim)\n",
    "print(oneHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "oneHotRe=oneHot.reshape(20000, 5)\n",
    "print(oneHotRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData,testDataDash=train_test_split(data,train_size=0.8,test_size=0.2,shuffle=False)\n",
    "trainLabel,testLabelDash=train_test_split(oneHotRe,train_size=0.8,test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validData,testData=train_test_split(testDataDash,train_size=0.5,test_size=0.5,shuffle=False)\n",
    "validLabel,testLabel=train_test_split(testLabelDash,train_size=0.5,test_size=0.5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "b1=np.zeros(h)\n",
    "b2=np.zeros(5)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.random.normal(0, 1, (20,h))\n",
    "w2=np.random.normal(0, 1, (h,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actFunc(data,choice):\n",
    "\n",
    "    if(choice==1):\n",
    "        return np.tanh(data)\n",
    "    elif(choice==2):\n",
    "        numerator=np.exp(data)\n",
    "        return numerator/np.sum(numerator,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunc(y,t):\n",
    "    return -(t*(np.log(y))+(1-t)*np.log(1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwFunc(data):\n",
    "    a0=data\n",
    "    z1=np.dot(a0,w1)+b1\n",
    "    a1=actFunc(z1,1)\n",
    "    z2=np.dot(a1,w2)+b2\n",
    "    a2=actFunc(z2,2)\n",
    "    return a0,z1,a1,z2,a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Acc(y,t,size):\n",
    "    maxData=np.argmax(y,axis=1)\n",
    "    maxLabel=np.argmax(t,axis=1)\n",
    "    compare=np.equal(maxData,maxLabel)\n",
    "    count=np.sum(compare)\n",
    "    return (count/size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 1.3360926734520475\n",
      "Training accuracy 73.1375\n",
      "Training Cost 1.0685016728695331\n",
      "Training accuracy 78.875\n",
      "Training Cost 0.9401474834529392\n",
      "Training accuracy 81.975\n",
      "Training Cost 0.8870572230564632\n",
      "Training accuracy 83.00625000000001\n",
      "Training Cost 0.843930172184744\n",
      "Training accuracy 83.6625\n",
      "Training Cost 0.8106265917266584\n",
      "Training accuracy 84.28750000000001\n",
      "Training Cost 0.7888949165870727\n",
      "Training accuracy 84.8375\n",
      "Training Cost 0.7651297108538834\n",
      "Training accuracy 85.375\n",
      "Training Cost 0.7527190762207108\n",
      "Training accuracy 85.46875\n",
      "Training Cost 0.7303304502333449\n",
      "Training accuracy 85.98125\n",
      "Training Cost 0.7279308265371006\n",
      "Training accuracy 86.0\n",
      "Training Cost 0.688887781073648\n",
      "Training accuracy 87.0625\n",
      "Training Cost 0.6953550204322305\n",
      "Training accuracy 86.8625\n",
      "Training Cost 0.6936150350970636\n",
      "Training accuracy 86.93125\n",
      "Training Cost 0.681301059052426\n",
      "Training accuracy 87.10625\n",
      "Training Cost 0.6739675408230308\n",
      "Training accuracy 87.25625000000001\n",
      "Training Cost 0.6676296150665337\n",
      "Training accuracy 87.325\n",
      "Training Cost 0.6605797754132913\n",
      "Training accuracy 87.41875\n",
      "Training Cost 0.6518374023191701\n",
      "Training accuracy 87.575\n",
      "Training Cost 0.6413563070317931\n",
      "Training accuracy 87.86874999999999\n",
      "Training Cost 0.628309137845835\n",
      "Training accuracy 88.21875\n",
      "Training Cost 0.6353306498579318\n",
      "Training accuracy 87.79375\n",
      "Training Cost 0.6391320914970048\n",
      "Training accuracy 87.51875\n",
      "Training Cost 0.6148538321616029\n",
      "Training accuracy 88.13125000000001\n",
      "Training Cost 0.6067760489864272\n",
      "Training accuracy 88.175\n",
      "Training Cost 0.601216971323925\n",
      "Training accuracy 88.325\n",
      "Training Cost 0.5963123376601104\n",
      "Training accuracy 88.51875\n",
      "Training Cost 0.5910890935777172\n",
      "Training accuracy 88.53750000000001\n",
      "Training Cost 0.5859167233222599\n",
      "Training accuracy 88.625\n",
      "Training Cost 0.5816811093884875\n",
      "Training accuracy 88.75\n",
      "Training Cost 0.5787007705320788\n",
      "Training accuracy 88.85625\n",
      "Training Cost 0.5765803253385618\n",
      "Training accuracy 88.89375000000001\n",
      "Training Cost 0.5748820311542076\n",
      "Training accuracy 88.97500000000001\n",
      "Training Cost 0.5733660424726965\n",
      "Training accuracy 89.125\n",
      "Training Cost 0.5718980537771468\n",
      "Training accuracy 89.1625\n",
      "Training Cost 0.5703927132418875\n",
      "Training accuracy 89.225\n",
      "Training Cost 0.5688113954002286\n",
      "Training accuracy 89.2625\n",
      "Training Cost 0.567149083405755\n",
      "Training accuracy 89.36874999999999\n",
      "Training Cost 0.5654043017245907\n",
      "Training accuracy 89.38125\n",
      "Training Cost 0.5635748144600315\n",
      "Training accuracy 89.43124999999999\n",
      "Training Cost 0.5616575729637434\n",
      "Training accuracy 89.4875\n",
      "Training Cost 0.5596333590790676\n",
      "Training accuracy 89.53125\n",
      "Training Cost 0.5574875800579712\n",
      "Training accuracy 89.53750000000001\n",
      "Training Cost 0.555339238021879\n",
      "Training accuracy 89.58749999999999\n",
      "Training Cost 0.553336805613924\n",
      "Training accuracy 89.61875\n",
      "Training Cost 0.5514227155545638\n",
      "Training accuracy 89.71249999999999\n",
      "Training Cost 0.5494940832569308\n",
      "Training accuracy 89.76875\n",
      "Training Cost 0.5475311285374015\n",
      "Training accuracy 89.74375\n",
      "Training Cost 0.5455461227176868\n",
      "Training accuracy 89.8375\n",
      "Training Cost 0.5435470714127303\n",
      "Training accuracy 89.9\n",
      "Training Cost 0.5415893268699086\n",
      "Training accuracy 89.9625\n",
      "Training Cost 0.5397793719691261\n",
      "Training accuracy 90.06875\n",
      "Training Cost 0.5381824891528583\n",
      "Training accuracy 90.13125\n",
      "Training Cost 0.5367764999964876\n",
      "Training accuracy 90.16875\n",
      "Training Cost 0.5355079031815778\n",
      "Training accuracy 90.20625\n",
      "Training Cost 0.5343307685605885\n",
      "Training accuracy 90.225\n",
      "Training Cost 0.5332066005205336\n",
      "Training accuracy 90.25\n",
      "Training Cost 0.5321058685543024\n",
      "Training accuracy 90.28125\n",
      "Training Cost 0.5310105540365225\n",
      "Training accuracy 90.28125\n",
      "Training Cost 0.5299151639331722\n",
      "Training accuracy 90.29374999999999\n",
      "Training Cost 0.5288292389550922\n",
      "Training accuracy 90.325\n",
      "Training Cost 0.5277754902673727\n",
      "Training accuracy 90.35625\n",
      "Training Cost 0.5267816050947653\n",
      "Training accuracy 90.4\n",
      "Training Cost 0.5258646342421783\n",
      "Training accuracy 90.41875\n",
      "Training Cost 0.5250176289469373\n",
      "Training accuracy 90.4625\n",
      "Training Cost 0.5242232771486219\n",
      "Training accuracy 90.49374999999999\n",
      "Training Cost 0.5234699987932027\n",
      "Training accuracy 90.4875\n",
      "Training Cost 0.5227510486539161\n",
      "Training accuracy 90.525\n",
      "Training Cost 0.522059758174288\n",
      "Training accuracy 90.5625\n",
      "Training Cost 0.5213868886808853\n",
      "Training accuracy 90.60000000000001\n",
      "Training Cost 0.5207185944480813\n",
      "Training accuracy 90.64999999999999\n",
      "Training Cost 0.520036311986464\n",
      "Training accuracy 90.64999999999999\n",
      "Training Cost 0.519317606733162\n",
      "Training accuracy 90.73125\n",
      "Training Cost 0.518540689767052\n",
      "Training accuracy 90.76875\n",
      "Training Cost 0.5177071981508922\n",
      "Training accuracy 90.79374999999999\n",
      "Training Cost 0.5168617178715014\n",
      "Training accuracy 90.77499999999999\n",
      "Training Cost 0.5160651273282897\n",
      "Training accuracy 90.79374999999999\n",
      "Training Cost 0.5153601637645627\n",
      "Training accuracy 90.8375\n",
      "Training Cost 0.5147607555102685\n",
      "Training accuracy 90.8375\n",
      "Training Cost 0.5142571634052518\n",
      "Training accuracy 90.85\n",
      "Training Cost 0.5138291281449376\n",
      "Training accuracy 90.83125\n",
      "Training Cost 0.5134563634460092\n",
      "Training accuracy 90.81875000000001\n",
      "Training Cost 0.5131225452135049\n",
      "Training accuracy 90.85625\n",
      "Training Cost 0.5128156541676245\n",
      "Training accuracy 90.8625\n",
      "Training Cost 0.5125274328881546\n",
      "Training accuracy 90.8625\n",
      "Training Cost 0.512252951670037\n",
      "Training accuracy 90.85625\n",
      "Training Cost 0.5119900435417916\n",
      "Training accuracy 90.88125000000001\n",
      "Training Cost 0.5117384529165772\n",
      "Training accuracy 90.90625\n",
      "Training Cost 0.5114990965648287\n",
      "Training accuracy 90.90625\n",
      "Training Cost 0.5112737702867676\n",
      "Training accuracy 90.89375\n",
      "Training Cost 0.5110653931355265\n",
      "Training accuracy 90.925\n",
      "Training Cost 0.5108788260490345\n",
      "Training accuracy 90.925\n",
      "Training Cost 0.5107223280824267\n",
      "Training accuracy 90.90625\n",
      "Training Cost 0.5106096231480045\n",
      "Training accuracy 90.925\n",
      "Training Cost 0.5105618063402682\n",
      "Training accuracy 90.9625\n",
      "Training Cost 0.5106054975496428\n",
      "Training accuracy 90.98125\n",
      "Training Cost 0.5107572973671222\n",
      "Training accuracy 90.925\n",
      "Training Cost 0.5109840820434811\n",
      "Training accuracy 90.9125\n",
      "Training Cost 0.5111731600515159\n",
      "Training accuracy 90.89375\n",
      "Training Cost 0.5112093107402905\n",
      "Training accuracy 90.90625\n",
      "Training Cost 0.5111025692954855\n",
      "Training accuracy 90.89375\n",
      "Training Cost 0.5109436007334947\n",
      "Training accuracy 90.875\n",
      "Training Cost 0.510766620196757\n",
      "Training accuracy 90.85\n",
      "Training Cost 0.5105324333652596\n",
      "Training accuracy 90.8875\n",
      "Training Cost 0.5102109794964846\n",
      "Training accuracy 90.8875\n",
      "Training Cost 0.5098197800860611\n",
      "Training accuracy 90.9\n",
      "Training Cost 0.5093806055896231\n",
      "Training accuracy 90.925\n",
      "Training Cost 0.5088879153196334\n",
      "Training accuracy 90.93124999999999\n",
      "Training Cost 0.5083231184398973\n",
      "Training accuracy 90.9625\n",
      "Training Cost 0.5076667033580234\n",
      "Training accuracy 91.01875\n",
      "Training Cost 0.5068901791428513\n",
      "Training accuracy 91.025\n",
      "Training Cost 0.5059647854210132\n",
      "Training accuracy 91.025\n",
      "Training Cost 0.5049202746299124\n",
      "Training accuracy 91.08125\n",
      "Training Cost 0.5038741196936017\n",
      "Training accuracy 91.0875\n",
      "Training Cost 0.5029289161567421\n",
      "Training accuracy 91.0875\n",
      "Training Cost 0.5021384738705703\n",
      "Training accuracy 91.03750000000001\n",
      "Training Cost 0.5014520868510713\n",
      "Training accuracy 91.03125\n",
      "Training Cost 0.5007267118784294\n",
      "Training accuracy 91.03125\n",
      "Training Cost 0.4999668827187404\n",
      "Training accuracy 91.03125\n",
      "Training Cost 0.4992493834463792\n",
      "Training accuracy 91.03750000000001\n",
      "Training Cost 0.49856834487119045\n",
      "Training accuracy 91.05\n",
      "Training Cost 0.49791533406046967\n",
      "Training accuracy 91.10000000000001\n",
      "Training Cost 0.4972939436406989\n",
      "Training accuracy 91.1375\n",
      "Training Cost 0.49670027395493244\n",
      "Training accuracy 91.15625\n",
      "Training Cost 0.49613465647002125\n",
      "Training accuracy 91.16250000000001\n",
      "Training Cost 0.4956040150483932\n",
      "Training accuracy 91.14375\n",
      "Training Cost 0.49511622071970324\n",
      "Training accuracy 91.10000000000001\n",
      "Training Cost 0.4946761053304447\n",
      "Training accuracy 91.10000000000001\n",
      "Training Cost 0.49428379078102974\n",
      "Training accuracy 91.1125\n",
      "Training Cost 0.49393515379111\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.4936236241555326\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.49334194959822364\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.49308323629630496\n",
      "Training accuracy 91.10000000000001\n",
      "Training Cost 0.4928414094840481\n",
      "Training accuracy 91.11875\n",
      "Training Cost 0.49261139714559726\n",
      "Training accuracy 91.13125\n",
      "Training Cost 0.4923891890971395\n",
      "Training accuracy 91.16250000000001\n",
      "Training Cost 0.4921718222157362\n",
      "Training accuracy 91.16875\n",
      "Training Cost 0.4919573112446415\n",
      "Training accuracy 91.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.4917445357276797\n",
      "Training accuracy 91.20625\n",
      "Training Cost 0.49153309280531476\n",
      "Training accuracy 91.20625\n",
      "Training Cost 0.49132313175604897\n",
      "Training accuracy 91.21249999999999\n",
      "Training Cost 0.49111519066491127\n",
      "Training accuracy 91.19375\n",
      "Training Cost 0.4909100465685939\n",
      "Training accuracy 91.19375\n",
      "Training Cost 0.4907085723557612\n",
      "Training accuracy 91.19375\n",
      "Training Cost 0.4905115911230975\n",
      "Training accuracy 91.21249999999999\n",
      "Training Cost 0.49031974377566395\n",
      "Training accuracy 91.21875\n",
      "Training Cost 0.4901334086110909\n",
      "Training accuracy 91.21875\n",
      "Training Cost 0.48995269498699584\n",
      "Training accuracy 91.2\n",
      "Training Cost 0.48977749170873824\n",
      "Training accuracy 91.21249999999999\n",
      "Training Cost 0.48960753121438594\n",
      "Training accuracy 91.20625\n",
      "Training Cost 0.4894424439524365\n",
      "Training accuracy 91.21875\n",
      "Training Cost 0.4892817978808576\n",
      "Training accuracy 91.23125\n",
      "Training Cost 0.4891251276404165\n",
      "Training accuracy 91.23750000000001\n",
      "Training Cost 0.48897195825975626\n",
      "Training accuracy 91.23125\n",
      "Training Cost 0.4888218259362705\n",
      "Training accuracy 91.26875\n",
      "Training Cost 0.4886742966802074\n",
      "Training accuracy 91.26875\n",
      "Training Cost 0.4885289827544862\n",
      "Training accuracy 91.26875\n",
      "Training Cost 0.48838555647171245\n",
      "Training accuracy 91.27499999999999\n",
      "Training Cost 0.4882437606890748\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.4881034151796955\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.48796441803986107\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.4878267415426974\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.48769042242248734\n",
      "Training accuracy 91.3\n",
      "Training Cost 0.48755554738565654\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.4874222354835269\n",
      "Training accuracy 91.27499999999999\n",
      "Training Cost 0.48729061958563324\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.4871608293550813\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.4870329777895588\n",
      "Training accuracy 91.27499999999999\n",
      "Training Cost 0.48690715266032486\n",
      "Training accuracy 91.27499999999999\n",
      "Training Cost 0.4867834132736609\n",
      "Training accuracy 91.26875\n",
      "Training Cost 0.48666179212465294\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.4865423003736233\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.486424935713275\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.48630969108635186\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.48619656278644585\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.4860855566445431\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.48597669121258075\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.48586999710187206\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.4857655119855245\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.48566327132185017\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.48556329561462486\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.48546557583034355\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.48537005910784053\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.4852766367982015\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.4851851360890626\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.4850953152601029\n",
      "Training accuracy 91.27499999999999\n",
      "Training Cost 0.48500686142702115\n",
      "Training accuracy 91.26875\n",
      "Training Cost 0.48491938881512575\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.48483243528732517\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.48474545500480065\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.48465780575008693\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.4845687309188946\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.4844773393136651\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.4843825920254269\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.4842833162780652\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.48417828023703824\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.48406637128806657\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.48394689814406394\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.4838199525303387\n",
      "Training accuracy 91.2875\n",
      "Training Cost 0.4836866382157083\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.4835489400776698\n",
      "Training accuracy 91.33125\n",
      "Training Cost 0.48340920484243377\n",
      "Training accuracy 91.375\n",
      "Training Cost 0.48326950338590985\n",
      "Training accuracy 91.36875\n",
      "Training Cost 0.48313120806400156\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.4829949012663094\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.48286051431657445\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.48272754827357106\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.48259528490188514\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.4824629559928483\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.4823298698079953\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.48219550363776614\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.4820595740390719\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.48192209523364965\n",
      "Training accuracy 91.375\n",
      "Training Cost 0.4817834286101078\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.48164431078263814\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.48150583191656104\n",
      "Training accuracy 91.3625\n",
      "Training Cost 0.48136933562837386\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.48123623728983245\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.48110780044933404\n",
      "Training accuracy 91.3625\n",
      "Training Cost 0.4809849425406527\n",
      "Training accuracy 91.38125000000001\n",
      "Training Cost 0.48086813189203137\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.4807573899698227\n",
      "Training accuracy 91.38125000000001\n",
      "Training Cost 0.48065236397920763\n",
      "Training accuracy 91.38125000000001\n",
      "Training Cost 0.4805524206284268\n",
      "Training accuracy 91.38125000000001\n",
      "Training Cost 0.48045672941664397\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.48036432741248764\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.48027417030854697\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.48018517654331483\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.4800962687219894\n",
      "Training accuracy 91.375\n",
      "Training Cost 0.48000641368825653\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.4799146606452919\n",
      "Training accuracy 91.38125000000001\n",
      "Training Cost 0.4798201757187338\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.4797222712845938\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.4796204291626697\n",
      "Training accuracy 91.3625\n",
      "Training Cost 0.47951431763474905\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.4794038017717581\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.47928894414728923\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.4791699906833437\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.47904733813043665\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.47892148769823484\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.4787930001359005\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.4786624726448398\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.4785305498985749\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.47839796132973905\n",
      "Training accuracy 91.38125000000001\n",
      "Training Cost 0.4782655580704971\n",
      "Training accuracy 91.4\n",
      "Training Cost 0.47813432234373615\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.47800533961917085\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.47787973755815666\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.47775859427081846\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.4776428221957658\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.47753305646146066\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.47742959128464696\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47733238861816835\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4772411465878006\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.47715539419260006\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4770745817525466\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4769981505915963\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47692557804499597\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4768564011788585\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47679022531527165\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.47672672327054233\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47666562970600335\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4766067332753727\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.476549867915824\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47649490384790494\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4764417385660825\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47639028815439266\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4763404794501929\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4762922437146087\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47624551242340546\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4762002155380809\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47615628222530293\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4761136435928193\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.476072236725771\n",
      "Training accuracy 91.49374999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.47603200920798483\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47599292339749893\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4759549599406115\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47591812027783725\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47588242814455534\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4758479302419697\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47581469631652806\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47578281883120405\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4757524122613556\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4757236118474248\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4756965714473713\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47567146001422866\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47564845622600854\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4756277409421646\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.475609487452655\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47559384989207815\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4755809506477004\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.47557086799707604\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4755636254474013\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.4755591842046248\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4755574398231922\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.475558223423585\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47556130706844657\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47556641217863116\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47557321945096237\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4755813787131809\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47559051748716036\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47560024759768715\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4756101698086594\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4756198770871428\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4756289576495126\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.47563699942322724\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.4756435979179415\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47564836960329177\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47565097249084737\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47565113441452817\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.475648687293892\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4756436025655379\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.4756360196377615\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4756262569363532\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4756147955299856\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4756022298814226\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.4755891891348587\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4755762433367687\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47556381739530756\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4755521361505981\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4755412145839329\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.47553089165523427\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.47552089232589495\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4755108965677663\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4755005976373787\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.4754897405405621\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47547814010232037\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4754656832193268\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.47545232139608906\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4754380588492266\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4754229397789346\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47540703679475993\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4753904413160309\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4753732560682399\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4753555894507375\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4753375514273072\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4753192505881234\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.475300792083597\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4752822761998093\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4752637974117625\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47524544380524675\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47522729679737596\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.47520943110988\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4751919149601745\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.4751748104365157\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47515817401914306\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4751420572031624\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4751265071742574\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47511156748725186\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.475097278700901\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4750836789297129\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47507080428388493\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47505868918004274\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.47504736651687\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4750368677197609\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4750272226666052\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47501845951250354\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47501060443465426\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47500368132019033\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47499771141970587\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4749927129879941\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47498870093144674\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4749856864789195\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4749836768899072\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47498267521079013\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4749826800869242\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4749836856356174\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4749856813827125\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47498865226370185\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4749925786890844\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4749974366730571\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4750031980245295\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4750098305997362\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4750172986161874\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4750255630280844\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.47503458196329484\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.4750443112211967\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.4750547048287956\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.47506571564916156\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.4750772960312419\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.4750893984834684\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.47510197634565754\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4751149844252969\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4751283795567632\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4751421210371553\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.475156170892361\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4751704939335956\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.4751850575790857\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47519983143739997\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4752147866758864\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4752298952256507\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4752451288983494\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4752604585048632\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47527585306840603\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4752912792141826\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.4753067007965366\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.47532207879684907\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4753373714963008\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.4753525349014706\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.475367523380666\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4753822904562116\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47539678969249144\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47541097562023404\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.47542480464274567\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47543823587796524\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47545123189993704\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47546375935349167\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47547578942584345\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47548729816789453\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4754982666659793\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4755086810713925\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.47551853250025894\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4755278168201251\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4755365343421976\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47554468943950634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 91.44375\n",
      "Training Cost 0.4755522901116344\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4755593475161666\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4755658754858369\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.47557189004866723\n",
      "Training accuracy 91.44375\n",
      "Training Cost 0.4755774089662573\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.4755824513029614\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47558703703596744\n",
      "Training accuracy 91.4375\n",
      "Training Cost 0.47559118671336215\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.475594921164121\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.47559826126068194\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.47560122773144464\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.47560384101738107\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.4756061211642331\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.47560808773981184\n",
      "Training accuracy 91.39375\n",
      "Training Cost 0.47560975976501113\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.4756111556474722\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.4756122931083797\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.47561318909544004\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.47561385967831854\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.47561431992627246\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.47561458377100513\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.47561466386057977\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.47561457141239705\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4756143160746829\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.4756139058066582\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.47561334678759204\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4756126433642989\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.47561179804532966\n",
      "Training accuracy 91.45625\n",
      "Training Cost 0.4756108115481476\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4756096829030096\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47560840961416856\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4756069878755469\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4756054128344015\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4756036788930307\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.4756017800355542\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47559971016460095\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.47559746343162723\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.47559503454475544\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47559241903951477\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47558961350053547\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4755866157258244\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47558342482932303\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47558004128157555\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.47557646689210215\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47557270474014957\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4755687590626783\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4755646351096805\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47556033897724104\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.4755558774283342\n",
      "Training accuracy 91.46875\n",
      "Training Cost 0.47555125771032186\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47554648737675836\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47554157411954645\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.4755365256159201\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47553134939326946\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4755260527135383\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47552064247786924\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47551512515135386\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.475509506707157\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.47550379258889225\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4754979876899176\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4754920963481295\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4754861223548584\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4754800689765403\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47547393898795814\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4754677347159563\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47546145809264045\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.47545511071713203\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.475448693924973\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47544220886423244\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4754356565772723\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.475429038086978\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.4754223544860615\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.47541560702780583\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.4754087972163859\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.4754019268946542\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47539499832709103\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4753880142754816\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.4753809780648428\n",
      "Training accuracy 91.50625\n",
      "Training Cost 0.4753738936371943\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.47536676559096924\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.47535959920419446\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.47535240044004073\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.4753451759339203\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.475337932961984\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.47533067939158646\n",
      "Training accuracy 91.525\n",
      "validation accuracy\n",
      "90.10000000000001\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for iteration in range(int(len(trainData)/batch)):\n",
    "        a0,z1,a1,z2,a2=forwFunc(trainData[iteration*batch:(iteration+1)*batch,:])\n",
    "        y=a2\n",
    "        labelBatch=trainLabel[iteration*batch:(iteration+1)*batch,:]\n",
    "        del2=(y-labelBatch)\n",
    "        del1=np.dot(del2,w2.T)*(1 - a1**2)\n",
    "        dcdw2=np.dot(a1.T,del2)\n",
    "        dcdw1=np.dot(a0.T,del1)\n",
    "        dcdb1=np.sum(del1,axis=0)\n",
    "        dcdb2=np.sum(del2,axis=0)\n",
    "        w1=w1-alpha*dcdw1\n",
    "        w2=w2-alpha*dcdw2\n",
    "        b2=b2-alpha*dcdb2\n",
    "        b1=b1-alpha*dcdb1\n",
    "    a0,z1,a1,z2,a2=forwFunc(trainData)\n",
    "    print(\"Training Cost\",(np.sum(costFunc(a2,trainLabel)))/16000.0)\n",
    "    print(\"Training accuracy\",Acc(a2,trainLabel,len(trainLabel)))\n",
    "    \n",
    "va0,vz1,va1,vz2,va2=forwFunc(validData)\n",
    "vOutput=va2\n",
    "accuracy = Acc(vOutput,validLabel,2000.0)\n",
    "print(\"validation accuracy\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
